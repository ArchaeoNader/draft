{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArchaeoNader/draft/blob/master/Another_copy_of_Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages Installation"
      ],
      "metadata": {
        "id": "omVog55b5VKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%python -m pip install --upgrade pip\n",
        "%pip install --upgrade pymupdf\n",
        "%pip install matplotlib"
      ],
      "metadata": {
        "id": "-khPfJ-T5W1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install nltk\n",
        "# https://www.nltk.org/\n",
        "\n",
        "#if you did not install the data to one of the above central locations, you will need to set the NLTK_DATA\n",
        "# https://www.nltk.org/data.html\n",
        "import nltk.data\n",
        "# from pathlib import Path\n",
        "# import sys\n",
        "# import os\n",
        "# from os import path\n",
        "\n",
        "# ntlkResourcesPath = path.join(Path(\"./Resources\"), \"ntlk_tokenizers\")\n",
        "# print( path.join(\"/Content\", ntlkResourcesPath))\n",
        "# os.environ['nltk_data'] = path.join(\"./Content\", ntlkResourcesPath)\n",
        "\n",
        "# Loading PunktSentenceTokenizer using English pickle file\n",
        "nltk.download('punkt', download_dir='/usr/share/nltk_data')\n",
        "nltk.download('punkt_tab', download_dir='/usr/share/nltk_data')\n",
        "tokenizer = nltk.data.load('nltk:tokenizers/punkt/PY3/english.pickle')"
      ],
      "metadata": {
        "id": "kcyYy7KE8BXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers==4.45.2\n",
        "%pip install sentence-transformers==2.2.2\n",
        "%pip install huggingface-hub==0.25.2\n",
        "%pip install torch\n",
        "%pip install InstructorEmbedding\n",
        "%pip install chromadb"
      ],
      "metadata": {
        "id": "ydJtBX1X7HBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb.api.types import Embeddings, Documents, EmbeddingFunction, Space\n",
        "from typing import List, Dict, Any, Optional\n",
        "import numpy as np\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "\n",
        "\n",
        "class InstructorEmbeddingFunction(EmbeddingFunction[Documents]):\n",
        "    \"\"\"\n",
        "    This class is used to generate embeddings for a list of texts using the Instructor embedding model.\n",
        "    \"\"\"\n",
        "\n",
        "    # If you have a GPU with at least 6GB try model_name = \"hkunlp/instructor-xl\" and device = \"cuda\"\n",
        "    # for a full list of options: https://github.com/HKUNLP/instructor-embedding#model-list\n",
        "    def __init__(\n",
        "        self,\n",
        "        instructor: INSTRUCTOR,\n",
        "        model_name: str = \"hkunlp/instructor-base\",\n",
        "        device: str = \"cpu\",\n",
        "        instruction: Optional[str] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the InstructorEmbeddingFunction.\n",
        "\n",
        "        Args:\n",
        "            model_name (str, optional): The name of the model to use for text embeddings.\n",
        "                Defaults to \"hkunlp/instructor-base\".\n",
        "            device (str, optional): The device to use for computation.\n",
        "                Defaults to \"cpu\".\n",
        "            instruction (str, optional): The instruction to use for the embeddings.\n",
        "                Defaults to None.\n",
        "        \"\"\"\n",
        "        # try:\n",
        "           # from InstructorEmbedding import INSTRUCTOR\n",
        "        # except ImportError:\n",
        "        #     raise ValueError(\n",
        "        #         \"The InstructorEmbedding python package is not installed. Please install it with `pip install InstructorEmbedding`\"\n",
        "        #     )\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.device = device\n",
        "        self.instruction = instruction\n",
        "\n",
        "        self._model = instructor #INSTRUCTOR(model_name_or_path=model_name, device=device)\n",
        "\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        \"\"\"\n",
        "        Generate embeddings for the given documents.\n",
        "\n",
        "        Args:\n",
        "            input: Documents or images to generate embeddings for.\n",
        "\n",
        "        Returns:\n",
        "            Embeddings for the documents.\n",
        "        \"\"\"\n",
        "        # Instructor only works with text documents\n",
        "        if not all(isinstance(item, str) for item in input):\n",
        "            raise ValueError(\"Instructor only supports text documents, not images\")\n",
        "\n",
        "        if self.instruction is None:\n",
        "            embeddings = self._model.encode(input, convert_to_numpy=True)\n",
        "        else:\n",
        "            texts_with_instructions = [[self.instruction, text] for text in input]\n",
        "            embeddings = self._model.encode(\n",
        "                texts_with_instructions, convert_to_numpy=True\n",
        "            )\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        return [np.array(embedding, dtype=np.float32) for embedding in embeddings]\n",
        "\n",
        "    @staticmethod\n",
        "    def name() -> str:\n",
        "        return \"instructor\"\n",
        "\n",
        "    def default_space(self) -> Space:\n",
        "        return \"cosine\"\n",
        "\n",
        "    def supported_spaces(self) -> List[Space]:\n",
        "        return [\"cosine\", \"l2\", \"ip\"]"
      ],
      "metadata": {
        "id": "9F3ht1z96cBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Huggingface text embedding model\n",
        "# https://huggingface.co/hkunlp/instructor-xl\n",
        "\n",
        "# Vector database engine\n",
        "# https://docs.trychroma.com/docs/overview/introduction\n",
        "\n",
        "db_dir = Path(\"./content/resources/chromadb\")\n",
        "embeddingModel_dir = Path(\"./content/resources/hkunlp_instructor_xl\")\n",
        "#shutil.rmtree(db_dir)\n",
        "#shutil.rmtree(embeddingModel_dir)\n",
        "\n",
        "instructor = INSTRUCTOR(\"hkunlp/instructor-xl\", cache_folder=embeddingModel_dir)\n",
        "ef = InstructorEmbeddingFunction(model_name=\"hkunlp/instructor-xl\", device=\"cuda\", instructor=instructor, instruction=\"Archaeology\")\n",
        "chroma_client = chromadb.PersistentClient(path=str(db_dir))\n",
        "collection = chroma_client.get_or_create_collection(name=\"archaeology\", embedding_function=ef )"
      ],
      "metadata": {
        "id": "mHHE8DLR8HS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install GitPython\n",
        "\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile\n",
        "from pathlib import Path\n",
        "from google.colab import userdata\n",
        "from git import Repo\n",
        "import shutil\n",
        "\n",
        "books_dir = Path(\"./content/resources/books\") #where the downloded files will be stored\n",
        "print(books_dir)\n",
        "\n",
        "if os.path.exists(books_dir) and listdir(books_dir) == []:\n",
        "  shutil.rmtree(books_dir)\n",
        "\n",
        "if os.path.exists(books_dir) is False:\n",
        "  #git_token = userdata.get('nader_token')\n",
        "  git_token = userdata.get('git_token')\n",
        "  git_user = userdata.get('git_user')\n",
        "  #repo = Repo.clone_from(str.format(\"https://{}:{}@github.com/Archaeonader/Books.git\", git_user, git_token), books_dir)\n",
        "  repo = Repo.clone_from(str.format(\"https://{}:{}@github.com/ibrahimkais/archpredec.git\", git_user, git_token), books_dir)\n",
        "\n",
        "\n",
        "files = listdir(books_dir)\n",
        "onlyfiles = [os.path.join(books_dir, f) for f in files if isfile(os.path.join(books_dir, f))]\n",
        "\n",
        "print(len(onlyfiles))\n",
        "print(onlyfiles)"
      ],
      "metadata": {
        "id": "2vh-lEL-8Obo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Cleaning and preprocessing function\n",
        "def clean_and_tokenize(text):\n",
        "    # Basic regex cleaning\n",
        "    text = re.sub(r'\\s\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = text.lower()  # Lowercase all text\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
        "    text = text.replace('\\n', ' ')\n",
        "    # Tokenize with gensim\n",
        "    #tokens = simple_preprocess(text)\n",
        "    #print(tokens)\n",
        "    return text #' '.join(tokens)\n",
        "\n",
        "\n",
        "# Chunk text into fixed-size chunks\n",
        "def chunk_text(text, chunk_size=200):\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "# Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0]  # Get token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
      ],
      "metadata": {
        "id": "TiWkutrO8RQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class PDFTextBlockCategorizer:\n",
        "    def __init__(self, blocks):\n",
        "        self.blocks = blocks\n",
        "\n",
        "    def run(self):\n",
        "        X = np.array(\n",
        "            [(x0, y0, x1, y1, len(text)) for x0, y0, x1, y1, text, page_index in self.blocks]\n",
        "        )\n",
        "\n",
        "        dbscan = DBSCAN()\n",
        "        dbscan.fit(X)\n",
        "        labels = dbscan.labels_\n",
        "        self.n_clusters = len(np.unique(labels))\n",
        "        label_counter = Counter(labels)\n",
        "        most_common_label = label_counter.most_common(1)[0][0]\n",
        "        labels = [0 if label == most_common_label else 1 for label in labels]\n",
        "        self.labels = labels"
      ],
      "metadata": {
        "id": "olscNBHspadQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade pymupdf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import fitz\n",
        "#from pathlib import Path\n",
        "from itertools import islice\n",
        "\n",
        "# https://github.com/pymupdf/PyMuPDF/discussions/2259\n",
        "class PDFExtractor:\n",
        "    pdf_root = \"\"\n",
        "\n",
        "    def __init__(self, fileName):\n",
        "        self.pdf_fullpath = fileName\n",
        "        self.pdf_doc = fitz.open(self.pdf_fullpath)\n",
        "\n",
        "    def calc_rect_center(self, rect, reverse_y=False):\n",
        "        if reverse_y:\n",
        "            x0, y0, x1, y1 = rect[0], -rect[1], rect[2], -rect[3]\n",
        "        else:\n",
        "            x0, y0, x1, y1 = rect\n",
        "\n",
        "        x_center = (x0 + x1) / 2\n",
        "        y_center = (y0 + y1) / 2\n",
        "        return (x_center, y_center)\n",
        "\n",
        "    def extract_all_text_blocks(self) -> list[(int,str)]:\n",
        "        # * https://pymupdf.readthedocs.io/en/latest/textpage.html#TextPage.extractBLOCKS\n",
        "\n",
        "        rect_centers = []\n",
        "        rects = []\n",
        "        visual_label_texts = []\n",
        "        categorize_vectors = []\n",
        "        text_blocks = []\n",
        "\n",
        "        for page_idx, page in islice(enumerate(self.pdf_doc), len(self.pdf_doc)):\n",
        "            blocks = page.get_text(\"blocks\")\n",
        "            page_cnt = page_idx + 1\n",
        "            # print(f\"=== Start Page {page_cnt}: {len(blocks)} blocks ===\")\n",
        "            block_cnt = 0\n",
        "            for block in blocks:\n",
        "                block_rect = block[:4]  # (x0,y0,x1,y1)\n",
        "                x0, y0, x1, y1 = block_rect\n",
        "                rects.append(block_rect)\n",
        "                block_text = block[4]\n",
        "                block_num = block[5]\n",
        "                # block_cnt += 1\n",
        "                block_cnt = block_num + 1\n",
        "\n",
        "                rect_center = self.calc_rect_center(block_rect, reverse_y=True)\n",
        "                rect_centers.append(rect_center)\n",
        "                # visual_label_text = f\"{block_text.split()[-1]}({page_cnt}.{block_cnt})\"\n",
        "                visual_label_text = f\"({page_cnt}.{block_cnt})\"\n",
        "                visual_label_texts.append(visual_label_text)\n",
        "\n",
        "                # block_type = \"text\" if block[6] == 0 else \"image\"\n",
        "                # print(f\"Block: {page_cnt}.{block_cnt}\")\n",
        "                # print(f\"<{block_type}> {rect_center} - {block_rect}\")\n",
        "                # print(block_text)\n",
        "\n",
        "                categorize_vectors.append((*block_rect, block_text, page_idx))\n",
        "\n",
        "            # print(f\"=== End Page {page_cnt}: {len(blocks)} blocks ===\\n\")\n",
        "\n",
        "        categorizer = PDFTextBlockCategorizer(categorize_vectors)\n",
        "        categorizer.run()\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        colors = [\"b\", \"r\", \"g\", \"c\", \"m\", \"y\", \"k\"]\n",
        "\n",
        "        for i, rect_center in enumerate(rect_centers):\n",
        "            label_idx = categorizer.labels[i]\n",
        "            color = colors[label_idx]\n",
        "            if color != \"r\":\n",
        "                text_blocks.append((categorize_vectors[i][5], categorize_vectors[i][4]))\n",
        "            x0, y0, x1, y1 = rects[i]\n",
        "            rect = Rectangle((x0, -y0), x1 - x0, -y1 + y0, fill=False, edgecolor=color)\n",
        "            ax.add_patch(rect)\n",
        "            x, y = rect_center\n",
        "            plt.scatter(x, y, color=color)\n",
        "            plt.annotate(visual_label_texts[i], rect_center)\n",
        "        plt.show()\n",
        "\n",
        "        return text_blocks"
      ],
      "metadata": {
        "id": "Y63gRab0pfnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyPDF2\n",
        "%pip install gensim\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "for book_index, file_name in enumerate(onlyfiles):\n",
        "    metadata = {}\n",
        "    pagesCount = 0\n",
        "    with open(file_name, \"rb\") as f:\n",
        "        reader = PdfReader(file_name)\n",
        "        for key in reader.metadata:\n",
        "            metadata[key.replace('/', '')] = reader.metadata[key]\n",
        "        pagesCount = len(reader.pages)\n",
        "\n",
        "    extractor = PDFExtractor(file_name)\n",
        "    text_blocks = extractor.extract_all_text_blocks()\n",
        "    for page_index, text in text_blocks:\n",
        "        metadata[\"PageIndex\"] = page_index + 1\n",
        "        text = clean_and_tokenize(text)\n",
        "\n",
        "        if len(text) > 0:\n",
        "            sentences = tokenizer.sentences_from_text(text)\n",
        "\n",
        "            for sent_index, sentence in enumerate(sentences):\n",
        "                tokens = simple_preprocess(sentence)\n",
        "                sentence = ' '.join(tokens)\n",
        "                chunked_documents = chunk_text(sentence)\n",
        "                print(str(page_index) + \" ['\" + str.join(\"', '\", chunked_documents) + \"']\")\n",
        "                # Generate embeddings for all document chunks in batches\n",
        "\n",
        "                if len(chunked_documents) > 0:\n",
        "                    metadatas = []\n",
        "                    ids = []\n",
        "                    for chunk_index, doc_chunk in enumerate(chunked_documents):\n",
        "                        metadatas.append(metadata)\n",
        "                        ids.append(str.format(\"{} {}{}{}\", metadata[\"Title\"], str(page_index).zfill(len(str(pagesCount))), sent_index, chunk_index))\n",
        "\n",
        "                    collection.add(\n",
        "                        documents = chunked_documents,\n",
        "                        metadatas = metadatas,\n",
        "                        ids = ids\n",
        "                    )"
      ],
      "metadata": {
        "id": "D_j_cpwQ8U0C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}